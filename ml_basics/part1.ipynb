{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Price Prediction - Regression Analysis\n",
    "\n",
    "This notebook implements a complete machine learning pipeline for predicting house prices using the Housing Prices Dataset from Kaggle.\n",
    "\n",
    "## Project Overview\n",
    "- **Problem Type**: Regression\n",
    "- **Target**: House Price\n",
    "- **Models**: Linear Regression, Random Forest, XGBoost\n",
    "- **Approach**: Simple preprocessing first, focus on prediction accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"yasserh/housing-prices-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "import os\n",
    "csv_files = [f for f in os.listdir(path) if f.endswith('.csv')]\n",
    "print(\"Available CSV files:\", csv_files)\n",
    "\n",
    "# Load the main dataset (assuming it's the first CSV file)\n",
    "data_file = os.path.join(path, csv_files[0])\n",
    "df = pd.read_csv(data_file)\n",
    "\n",
    "print(f\"Dataset loaded: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic information about the dataset\n",
    "print(\"Dataset Info:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "print(f\"\\nMissing values:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "print(\"Statistical Summary:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify target variable (assuming it's 'price' or similar)\n",
    "# Let's check column names first\n",
    "print(\"Column names:\")\n",
    "for i, col in enumerate(df.columns):\n",
    "    print(f\"{i}: {col}\")\n",
    "\n",
    "# Identify potential target variable\n",
    "price_columns = [col for col in df.columns if 'price' in col.lower()]\n",
    "print(f\"\\nPotential target columns: {price_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set target variable (adjust this based on actual column name)\n",
    "target_col = price_columns[0] if price_columns else df.columns[-1]\n",
    "print(f\"Using '{target_col}' as target variable\")\n",
    "\n",
    "# Visualize target distribution\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(df[target_col], bins=50, alpha=0.7)\n",
    "plt.title(f'Distribution of {target_col}')\n",
    "plt.xlabel(target_col)\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(df[target_col])\n",
    "plt.title(f'Boxplot of {target_col}')\n",
    "plt.ylabel(target_col)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Target variable stats:\")\n",
    "print(f\"Mean: ${df[target_col].mean():,.2f}\")\n",
    "print(f\"Median: ${df[target_col].median():,.2f}\")\n",
    "print(f\"Std: ${df[target_col].std():,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for preprocessing\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Identify numerical and categorical columns\n",
    "numerical_cols = df_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = df_clean.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Remove target from numerical columns if it's there\n",
    "if target_col in numerical_cols:\n",
    "    numerical_cols.remove(target_col)\n",
    "\n",
    "print(f\"Numerical columns ({len(numerical_cols)}): {numerical_cols}\")\n",
    "print(f\"Categorical columns ({len(categorical_cols)}): {categorical_cols}\")\n",
    "print(f\"Target column: {target_col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove extreme outliers from target variable (top and bottom 1%)\n",
    "Q1 = df_clean[target_col].quantile(0.01)\n",
    "Q99 = df_clean[target_col].quantile(0.99)\n",
    "\n",
    "print(f\"Before outlier removal: {len(df_clean)} rows\")\n",
    "print(f\"Removing values below ${Q1:,.2f} and above ${Q99:,.2f}\")\n",
    "\n",
    "df_clean = df_clean[(df_clean[target_col] >= Q1) & (df_clean[target_col] <= Q99)]\n",
    "print(f\"After outlier removal: {len(df_clean)} rows ({len(df) - len(df_clean)} removed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "print(\"Missing values before cleaning:\")\n",
    "missing_before = df_clean.isnull().sum()\n",
    "print(missing_before[missing_before > 0])\n",
    "\n",
    "# Fill numerical missing values with median\n",
    "for col in numerical_cols:\n",
    "    if df_clean[col].isnull().sum() > 0:\n",
    "        df_clean[col].fillna(df_clean[col].median(), inplace=True)\n",
    "        print(f\"Filled {col} with median: {df_clean[col].median():.2f}\")\n",
    "\n",
    "# Fill categorical missing values with mode\n",
    "for col in categorical_cols:\n",
    "    if df_clean[col].isnull().sum() > 0:\n",
    "        mode_value = df_clean[col].mode()[0] if len(df_clean[col].mode()) > 0 else 'Unknown'\n",
    "        df_clean[col].fillna(mode_value, inplace=True)\n",
    "        print(f\"Filled {col} with mode: {mode_value}\")\n",
    "\n",
    "print(f\"\\nMissing values after cleaning: {df_clean.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables\n",
    "df_encoded = df_clean.copy()\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    # Use label encoding for simplicity (can switch to one-hot if needed)\n",
    "    le = LabelEncoder()\n",
    "    df_encoded[col] = le.fit_transform(df_encoded[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "    print(f\"Encoded {col}: {len(le.classes_)} unique categories\")\n",
    "\n",
    "print(f\"\\nDataset shape after encoding: {df_encoded.shape}\")\n",
    "print(\"All columns are now numerical!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature-Target Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation with target\n",
    "correlations = df_encoded.corr()[target_col].sort_values(key=abs, ascending=False)\n",
    "correlations = correlations.drop(target_col)  # Remove self-correlation\n",
    "\n",
    "print(\"Top 10 features most correlated with target:\")\n",
    "print(correlations.head(10))\n",
    "\n",
    "# Plot correlation\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_features = correlations.head(15)\n",
    "plt.barh(range(len(top_features)), top_features.values)\n",
    "plt.yticks(range(len(top_features)), top_features.index)\n",
    "plt.xlabel('Correlation with Target')\n",
    "plt.title('Top 15 Features Correlated with House Price')\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "X = df_encoded.drop(target_col, axis=1)\n",
    "y = df_encoded[target_col]\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "\n",
    "# Split into train, validation, and test sets (70/15/15)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.176, random_state=42)  # 0.176 ≈ 0.15/0.85\n",
    "\n",
    "print(f\"\\nData splits:\")\n",
    "print(f\"Train: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Validation: {X_val.shape[0]} samples ({X_val.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Test: {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Features scaled using StandardScaler\")\n",
    "print(f\"Train mean: {X_train_scaled.mean():.3f}, std: {X_train_scaled.std():.3f}\")\n",
    "print(f\"Validation mean: {X_val_scaled.mean():.3f}, std: {X_val_scaled.std():.3f}\")\n",
    "print(f\"Test mean: {X_test_scaled.mean():.3f}, std: {X_test_scaled.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Development and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation function\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"\\n{model_name} Results:\")\n",
    "    print(f\"MAE: ${mae:,.2f}\")\n",
    "    print(f\"MSE: ${mse:,.2f}\")\n",
    "    print(f\"RMSE: ${rmse:,.2f}\")\n",
    "    print(f\"R²: {r2:.4f}\")\n",
    "    \n",
    "    return {'MAE': mae, 'MSE': mse, 'RMSE': rmse, 'R2': r2}\n",
    "\n",
    "# Store results\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Linear Regression (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Linear Regression\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "lr_train_pred = lr_model.predict(X_train_scaled)\n",
    "lr_val_pred = lr_model.predict(X_val_scaled)\n",
    "\n",
    "# Evaluate\n",
    "print(\"=== LINEAR REGRESSION ===\")\n",
    "results['Linear_Regression_Train'] = evaluate_model(y_train, lr_train_pred, \"Linear Regression (Train)\")\n",
    "results['Linear_Regression_Val'] = evaluate_model(y_val, lr_val_pred, \"Linear Regression (Validation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest (using original unscaled features as RF doesn't require scaling)\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "rf_train_pred = rf_model.predict(X_train)\n",
    "rf_val_pred = rf_model.predict(X_val)\n",
    "\n",
    "# Evaluate\n",
    "print(\"=== RANDOM FOREST ===\")\n",
    "results['Random_Forest_Train'] = evaluate_model(y_train, rf_train_pred, \"Random Forest (Train)\")\n",
    "results['Random_Forest_Val'] = evaluate_model(y_val, rf_val_pred, \"Random Forest (Validation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and import XGBoost\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "except ImportError:\n",
    "    print(\"Installing XGBoost...\")\n",
    "    !pip install xgboost\n",
    "    import xgboost as xgb\n",
    "\n",
    "# Train XGBoost\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "xgb_train_pred = xgb_model.predict(X_train)\n",
    "xgb_val_pred = xgb_model.predict(X_val)\n",
    "\n",
    "# Evaluate\n",
    "print(\"=== XGBOOST ===\")\n",
    "results['XGBoost_Train'] = evaluate_model(y_train, xgb_train_pred, \"XGBoost (Train)\")\n",
    "results['XGBoost_Val'] = evaluate_model(y_val, xgb_val_pred, \"XGBoost (Validation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Comparison and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame(results).T\n",
    "comparison_df['Model'] = ['LR_Train', 'LR_Val', 'RF_Train', 'RF_Val', 'XGB_Train', 'XGB_Val']\n",
    "comparison_df = comparison_df.set_index('Model')\n",
    "\n",
    "print(\"\\n=== MODEL COMPARISON ===\")\n",
    "print(comparison_df.round(2))\n",
    "\n",
    "# Extract validation results only\n",
    "val_results = comparison_df[comparison_df.index.str.contains('Val')].copy()\n",
    "val_results.index = ['Linear Regression', 'Random Forest', 'XGBoost']\n",
    "\n",
    "print(\"\\n=== VALIDATION SET PERFORMANCE ===\")\n",
    "print(val_results.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "metrics = ['MAE', 'RMSE', 'R2']\n",
    "models = val_results.index\n",
    "\n",
    "# MAE\n",
    "axes[0,0].bar(models, val_results['MAE'])\n",
    "axes[0,0].set_title('Mean Absolute Error (Lower is Better)')\n",
    "axes[0,0].set_ylabel('MAE ($)')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# RMSE\n",
    "axes[0,1].bar(models, val_results['RMSE'])\n",
    "axes[0,1].set_title('Root Mean Squared Error (Lower is Better)')\n",
    "axes[0,1].set_ylabel('RMSE ($)')\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# R²\n",
    "axes[1,0].bar(models, val_results['R2'])\n",
    "axes[1,0].set_title('R² Score (Higher is Better)')\n",
    "axes[1,0].set_ylabel('R²')\n",
    "axes[1,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Best model summary\n",
    "best_mae_model = val_results['MAE'].idxmin()\n",
    "best_r2_model = val_results['R2'].idxmax()\n",
    "\n",
    "axes[1,1].text(0.1, 0.8, 'Best Model Summary:', fontsize=14, fontweight='bold', transform=axes[1,1].transAxes)\n",
    "axes[1,1].text(0.1, 0.6, f'Lowest MAE: {best_mae_model}', fontsize=12, transform=axes[1,1].transAxes)\n",
    "axes[1,1].text(0.1, 0.5, f'MAE: ${val_results.loc[best_mae_model, \"MAE\"]:,.0f}', fontsize=12, transform=axes[1,1].transAxes)\n",
    "axes[1,1].text(0.1, 0.3, f'Highest R²: {best_r2_model}', fontsize=12, transform=axes[1,1].transAxes)\n",
    "axes[1,1].text(0.1, 0.2, f'R²: {val_results.loc[best_r2_model, \"R2\"]:.3f}', fontsize=12, transform=axes[1,1].transAxes)\n",
    "axes[1,1].set_xlim(0, 1)\n",
    "axes[1,1].set_ylim(0, 1)\n",
    "axes[1,1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Model Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model based on validation R²\n",
    "best_model_name = val_results['R2'].idxmax()\n",
    "print(f\"Best model based on validation R²: {best_model_name}\")\n",
    "\n",
    "# Get the corresponding trained model\n",
    "if best_model_name == 'Linear Regression':\n",
    "    best_model = lr_model\n",
    "    test_pred = best_model.predict(X_test_scaled)\n",
    "elif best_model_name == 'Random Forest':\n",
    "    best_model = rf_model\n",
    "    test_pred = best_model.predict(X_test)\n",
    "else:  # XGBoost\n",
    "    best_model = xgb_model\n",
    "    test_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate on test set\n",
    "print(f\"\\n=== FINAL TEST SET EVALUATION ({best_model_name}) ===\")\n",
    "test_results = evaluate_model(y_test, test_pred, f\"{best_model_name} (Test Set)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction vs Actual Plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Scatter plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(y_test, test_pred, alpha=0.6)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Price')\n",
    "plt.ylabel('Predicted Price')\n",
    "plt.title(f'{best_model_name}: Predictions vs Actual')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals plot\n",
    "plt.subplot(1, 2, 2)\n",
    "residuals = y_test - test_pred\n",
    "plt.scatter(test_pred, residuals, alpha=0.6)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Predicted Price')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title(f'{best_model_name}: Residual Plot')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance (works for RF and XGBoost)\n",
    "if best_model_name in ['Random Forest', 'XGBoost']:\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    top_15_features = feature_importance.head(15)\n",
    "    plt.barh(range(len(top_15_features)), top_15_features['importance'])\n",
    "    plt.yticks(range(len(top_15_features)), top_15_features['feature'])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title(f'{best_model_name}: Top 15 Most Important Features')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Top 10 Most Important Features:\")\n",
    "    print(feature_importance.head(10))\n",
    "    \n",
    "elif best_model_name == 'Linear Regression':\n",
    "    # For linear regression, show coefficients\n",
    "    coef_df = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'coefficient': lr_model.coef_\n",
    "    })\n",
    "    coef_df['abs_coefficient'] = abs(coef_df['coefficient'])\n",
    "    coef_df = coef_df.sort_values('abs_coefficient', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    top_15_coef = coef_df.head(15)\n",
    "    colors = ['red' if x < 0 else 'blue' for x in top_15_coef['coefficient']]\n",
    "    plt.barh(range(len(top_15_coef)), top_15_coef['coefficient'], color=colors)\n",
    "    plt.yticks(range(len(top_15_coef)), top_15_coef['feature'])\n",
    "    plt.xlabel('Coefficient Value')\n",
    "    plt.title('Linear Regression: Top 15 Feature Coefficients')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Top 10 Features by Absolute Coefficient:\")\n",
    "    print(coef_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"                    PROJECT SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n📊 Dataset Information:\")\n",
    "print(f\"   • Original size: {df.shape[0]:,} samples, {df.shape[1]} features\")\n",
    "print(f\"   • After cleaning: {len(df_clean):,} samples ({(len(df_clean)/len(df)*100):.1f}% retained)\")\n",
    "print(f\"   • Features used: {X.shape[1]} (after encoding)\")\n",
    "\n",
    "print(f\"\\n🎯 Target Variable ({target_col}):\")\n",
    "print(f\"   • Mean: ${df_clean[target_col].mean():,.0f}\")\n",
    "print(f\"   • Median: ${df_clean[target_col].median():,.0f}\")\n",
    "print(f\"   • Range: ${df_clean[target_col].min():,.0f} - ${df_clean[target_col].max():,.0f}\")\n",
    "\n",
    "print(f\"\\n🏆 Best Model: {best_model_name}\")\n",
    "print(f\"   • Test R²: {test_results['R2']:.4f}\")\n",
    "print(f\"   • Test RMSE: ${test_results['RMSE']:,.0f}\")\n",
    "print(f\"   • Test MAE: ${test_results['MAE']:,.0f}\")\n",
    "\n",
    "print(f\"\\n📈 Model Performance Comparison (Validation):\")\n",
    "for model in val_results.index:\n",
    "    r2 = val_results.loc[model, 'R2']\n",
    "    mae = val_results.loc[model, 'MAE']\n",
    "    print(f\"   • {model}: R² = {r2:.4f}, MAE = ${mae:,.0f}\")\n",
    "\n",
    "print(f\"\\n💡 Key Insights:\")\n",
    "print(f\"   • Model explains {test_results['R2']*100:.1f}% of price variance\")\n",
    "print(f\"   • Average prediction error: ${test_results['MAE']:,.0f}\")\n",
    "print(f\"   • Data preprocessing successfully handled missing values and outliers\")\n",
    "print(f\"   • Simple approaches work well - complex feature engineering may not be needed\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"                   PROJECT COMPLETE ✅\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}